---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
  labels:
    component.kutara.io/part-of: standard-helm-release
spec:
  interval: 5m
  upgrade:
    remediation:
      remediateLastFailure: false
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.13.7
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  dependsOn:
    - name: rook-ceph
  values:
    toolbox:
      enabled: false
      image: quay.io/ceph/ceph:v18.2.2@sha256:9d7bcfea8d18999ed9e00e9c9d124f9ff14a1602e92486da20752c2a40a6c07f
    monitoring:
      enabled: true
    cephClusterSpec:
      # This is a terrible hack
      # I need more resources.
      resources:
        osd:
          limits:
            cpu: "2000m"
            memory: "4Gi"
          requests:
            cpu: "500m"
            memory: "4Gi"
      network:
        provider: host
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
      placement:
        mgr:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: In
                    values:
                    - ""
          tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
        mon:
          count: 3
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: In
                    values:
                    - ""
          tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
    cephBlockPoolsVolumeSnapshotClass:
      enabled: false
    cephBlockPools:
      - name: fast-ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
          deviceClass: ssd
        storageClass:
          enabled: true
          name: fast-ceph-block
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
      - name: builtin-mgr
        spec:
          name: .mgr
          failureDomain: host
          replicated:
            size: 3
            requireSafeReplicaSize: true
          deviceClass: ssd
          parameters:
            compression_mode: none
        storageClass:
          enabled: false
      - name: slow-ceph-blockpool
        spec:
          failureDomain: osd
          replicated:
            size: 2
            # requireSafeReplicaSize: false
          deviceClass: hdd
        storageClass:
          enabled: true
          name: slow-ceph-block
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephFileSystems:
      - name: fast-ceph-filesystem
        spec:
          metadataPool:
            deviceClass: ssd
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
              name: fast-data0
              deviceClass: ssd
          metadataServer:
            activeCount: 1
            activeStandby: true
            placement:
             nodeAffinity:
               requiredDuringSchedulingIgnoredDuringExecution:
                 nodeSelectorTerms:
                 - matchExpressions:
                   - key: node-role.kubernetes.io/control-plane
                     operator: In
                     values:
                     - ""
             tolerations:
             - key: node-role.kubernetes.io/control-plane
               operator: Exists
            resources:
              limits:
                cpu: "2000m"
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "4Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: fast-ceph-filesystem
          pool: fast-data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
      - name: slow-ceph-filesystem
        spec:
          metadataPool:
            deviceClass: ssd
            replicated:
              size: 3
          dataPools:
            - failureDomain: osd
              replicated:
                size: 2
              name: slow-data0
              deviceClass: hdd
          metadataServer:
            activeCount: 1
            activeStandby: true
            placement:
             nodeAffinity:
               requiredDuringSchedulingIgnoredDuringExecution:
                 nodeSelectorTerms:
                 - matchExpressions:
                   - key: node-role.kubernetes.io/control-plane
                     operator: In
                     values:
                     - ""
             tolerations:
             - key: node-role.kubernetes.io/control-plane
               operator: Exists
            resources:
              limits:
                cpu: "2000m"
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "4Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: slow-ceph-filesystem
          pool: slow-data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephObjectStores:
      # Having two object stores breaks PG Autoscaler
      # For now just run one on spinning rust.
      # - name: fast-ceph-objectstore
      #   spec:
      #     metadataPool:
      #       failureDomain: host
      #       replicated:
      #         size: 3
      #     dataPool:
      #       failureDomain: host
      #       deviceClass: ssd
      #       erasureCoded:
      #         dataChunks: 2
      #         codingChunks: 1
      #     preservePoolsOnDelete: true
      #     gateway:
      #       port: 80
      #       placement:
      #        nodeAffinity:
      #          requiredDuringSchedulingIgnoredDuringExecution:
      #            nodeSelectorTerms:
      #            - matchExpressions:
      #              - key: feature.node.kubernetes.io/custom-intel-10g
      #                operator: In
      #                values:
      #                - "true"
      #       resources:
      #         limits:
      #           cpu: "2000m"
      #           memory: "2Gi"
      #         requests:
      #           cpu: "1000m"
      #           memory: "1Gi"
      #       instances: 1
      #       priorityClassName: system-cluster-critical
      #   storageClass:
      #     enabled: true
      #     name: fast-ceph-bucket
      #     reclaimPolicy: Delete
      #     parameters:
      #       region: us-east-1
      - name: slow-ceph-objectstore
        spec:
          metadataPool:
            deviceClass: ssd
            failureDomain: host
            replicated:
              size: 2
          dataPool:
            failureDomain: osd
            deviceClass: hdd
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          preservePoolsOnDelete: false
          gateway:
            port: 80
            hostNetwork: false
            placement:
             nodeAffinity:
               requiredDuringSchedulingIgnoredDuringExecution:
                 nodeSelectorTerms:
                 - matchExpressions:
                   - key: feature.node.kubernetes.io/custom-intel-10g
                     operator: In
                     values:
                     - "true"
            resources:
              limits:
                cpu: "2000m"
                memory: "2Gi"
              requests:
                cpu: "1000m"
                memory: "1Gi"
            instances: 1
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          name: slow-ceph-bucket
          reclaimPolicy: Delete
          parameters:
            region: us-east-1
    ingress:
      dashboard:
        ingressClassName: "nginx"
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-production
        host:
          name: "rook.cluster-0.scr1.rabbito.tech"
          path: "/"
        tls:
          - secretName: rook-tls
            hosts:
              - "rook.cluster-0.scr1.rabbito.tech"
